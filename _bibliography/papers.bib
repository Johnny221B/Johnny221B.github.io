---
---

@string{aps = {American Physical Society,}}


@misc{yang2024ucfeusercentricfinancialexpertise,
      title={UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models}, 
      author={Yuzhe Yang* and Yifei Zhang* and Yan Hu and Yilin Guo and Ruoli Gan and Yueru He and Mingcong Lei and Xiao Zhang and Haining Wang and Qianqian Xie and Jimin Huang and Honghai Yu and Benyou Wang},
      year={2024},
      journal={arXiv preprint},
      arxiv={2410.14059},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP},
      url={https://arxiv.org/abs/2410.14059}, 
      pdf={https://arxiv.org/pdf/2410.14059},
      preview={UCFE.png},
      code={https://github.com/TobyYang7/UCFE-Benchmark},
      abstract={This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, taskspecific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available.},
      note={A User-Centric framework designed to evaluate LLMs' ability to handle complex financial tasks},
      selected={true},
      bibtex_show={false}
}


@article{LI2024102326,
title = {FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction},
journal = {Information Fusion},
pages = {102326},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102326},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001040},
html = {https://www.sciencedirect.com/science/article/pii/S1566253524001040},
preview = {FAST-CA.jpg},
pdf = {FAST-CA.pdf},
author = {Chi Li and Xixian Qi and Yuzhe Yang and Zhuo Zeng and Lianmin Zhang and Jianfeng Mao},
keywords = {Graph neural networks, Flight delay prediction, Delay propagation, Dynamic graph, Adaptive learning},
abstract = {The issue of delay propagation prediction in airport networks has garnered increasing global attention, particularly due to its profound impact on operational efficiency and passenger satisfaction in modern air transportation systems. Despite research advancements in this domain, existing methodologies often fall short of comprehensively addressing the challenges associated with predicting delay propagation in airport networks, especially in terms of handling complex spatial-temporal dependencies and sequence couplings. In response to the complex challenge of predicting delay propagation in airport networks, we introduce the Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention (FAST-CA) framework. FAST-CA is an innovative model that integrates dynamic and adaptive graph learning, coupled attention mechanisms, periodicity feature extraction, and multifaceted information fusion modules. This holistic approach enables a thorough analysis of the interplay between flight departure and arrival delays and the spatial-temporal correlations within airport networks. Rigorously evaluated on two extensive real-world datasets, our model consistently outperforms current state-of-the-art baseline models, showcasing superior predictive performance and the effective learning capabilities of its intricately designed modules. Our research highlights the criticality of analyzing spatial-temporal relationships and the dynamics of flight coupling, offering significant theoretical and practical contributions to the advancement and management of air transportation systems.},
selected={true},
bibtex_show={false},
note={SOTA spatio-temporal model for predicting airport network delay propagation}
}

@article{xie2024openfinllmsopenmultimodallarge,
      abbr={arxiv},
      bibtex_show={false},
      title={Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications}, 
      journal={arXiv preprint},
      year={2024},
      selected={true},
      pdf={https://arxiv.org/pdf/2408.11878},
      arxiv={2408.11878}, 
      author={Qianqian Xie and Dong Li and Mengxi Xiao and Zihao Jiang and Ruoyu Xiang and Xiao Zhang and Zhengyu Chen and Yueru He and Weiguang Han and Yuzhe Yang and Shunian Chen and Yifei Zhang and Lihang Shen and Daniel Kim and Zhiwei Liu and Zheheng Luo and Yangyang Yu and Yupeng Cao and Zhiyang Deng and Zhiyuan Yao and Haohang Li and Duanyu Feng and Yongfu Dai and VijayaSai Somasundaram and Peng Lu and Yilun Zhao and Yitao Long and Guojun Xiong and Kaleb Smith and Honghai Yu and Yanzhao Lai and Min Peng and Jianyun Nie and Jordan W. Suchow and Xiao-Yang Liu and Benyou Wang and Alejandro Lopez-Lira and Jimin Huang and Sophia Ananiadou},
      abstract={Large language models (LLMs) have advanced financial applications, yet they often lack sufficient financial knowledge and struggle with tasks involving multi-modal inputs like tables and time series data. To address these limitations, we introduce Open-FinLLMs, a series of Financial LLMs. We begin with FinLLaMA, pre-trained on a 52 billion token financial corpus, incorporating text, tables, and time-series data to embed comprehensive financial knowledge. FinLLaMA is then instruction fine-tuned with 573K financial instructions, resulting in FinLLaMA-instruct, which enhances task performance. Finally, we present FinLLaVA, a multimodal LLM trained with 1.43M image-text instructions to handle complex financial data types. Extensive evaluations demonstrate FinLLaMA's superior performance over LLaMA3-8B, LLaMA3.1-8B, and BloombergGPT in both zero-shot and few-shot settings across 19 and 4 datasets, respectively. FinLLaMA-instruct outperforms GPT-4 and other Financial LLMs on 15 datasets. FinLLaVA excels in understanding tables and charts across 4 multimodal tasks. Additionally, FinLLaMA achieves impressive Sharpe Ratios in trading simulations, highlighting its robust financial application capabilities. We will continually maintain and improve our models and benchmarks to support ongoing innovation in academia and industry.},
      note={First open-source financial multimodal LLM: FinLLaVA-8B},
      website={https://www.thefin.ai/home},
      preview={finllava.png},
      html={https://huggingface.co/papers/2408.11878}
}

@article{wu2024feddtpt,
  title={FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models},
  author={Wu, Jiaqi and Chen, Simin and Yang, Yuzhe and Li, Yijiang and Hou, Shiyue and Jing, Rui and Wang, Zehua and Chen, Wei and Tian, Zijian},
  year={2024},
  abstract={In recent years, large language models (LLMs) have significantly advanced the field of natural language processing (NLP). By fine-tuning LLMs with data from specific scenarios, these foundation models can better adapt to various downstream tasks. However, the fine-tuning process poses privacy leakage risks, particularly in centralized data processing scenarios. To address user privacy concerns, federated learning (FL) has been introduced to mitigate the risks associated with centralized data collection from multiple sources. Nevertheless, the privacy of LLMs themselves is equally critical, as potential malicious attacks challenge their security, an issue that has received limited attention in current research. Consequently, establishing a trusted multi-party model fine-tuning environment is essential. Additionally, the local deployment of large LLMs incurs significant storage costs and high computational demands. To address these challenges, we propose for the first time a federated discrete and transferable prompt tuning, namely FedDTPT, for black-box large language models. In the client optimization phase, we adopt a token-level discrete prompt optimization method that leverages a feedback loop based on prediction accuracy to drive gradient-free prompt optimization through the MLM API. For server optimization, we employ an attention mechanism based on semantic similarity to filter all local prompt tokens, along with an embedding distance elbow detection and DBSCAN clustering strategy to enhance the filtering process. Experimental results demonstrate that, compared to state-of-the-art methods, our approach achieves higher accuracy, reduced communication overhead, and robustness to non-iid data in a black-box setting. Moreover, the optimized prompts are transferable.},
  preview={feddtpt.png},
  selected={true},
  pdf={FedDTPT.pdf},
}

@misc{yang2024quantgpt,
  author       = {Yuzhe Yang and Kangqi Yu and Juanquan Peng},
  title        = {Quant-GPT: Money is All You Need},
  year         = {2024},
  code          = {https://github.com/TobyYang7/Quant-GPT},
  poster={Quant-GPT_poster.pdf},
  preview={Quant-GPT.png},
  pdf={Quant_GPT_final_report.pdf},
  note={A Large Language Model for A-share Market Investment},
  abstract={This paper introduces Quant-GPT, a novel multi-agent optimized for Ashare market investment decisions. Leveraging a fine-tuning combination of
distilled sentiment analysis from ChatGPT and real-world market data, the
prediction agent of Quant-GPT addresses the challenges of model collapse
and weak causality between sentiment and expected returns. Our methodology integrates a Retrieval-Augmented Generation (RAG) agent and summary agent, enabling the model to access relevant news articles and corporate announcements summary with concise investment information to enhance investment decision-making. The inclusion of diverse datasets and
RAG significantly improves the model’s ability to forecast market trends and
returns accurately. Experimental results demonstrate Quant-GPT’s superior performance over existing open-source LLMs in terms of annualized return, maximum draw-down, and Sharpe ratio. These findings underscore
the potential of advanced language models in financial applications, providing a robust framework for integrating natural language understanding
with quantitative investment strategies. The code is available on GitHub:
https://github.com/TobyYang7/Quant-GPT}
}

@misc{llava_qwen2,
  author       = {Yuzhe Yang},
  title        = {LLaVA-Qwen2: Enhanced with Qwen2 Base Model},
  year         = {2024},
  code = {https://github.com/TobyYang7/Llava_Qwen2},
  note         = {An extension of LLaVA integrating the Qwen2 model for enhanced visual instruction tuning.},
  preview={Llava_Qwen2.png},
  abstract     = {LLaVA-Qwen2 enhances the LLaVA project by incorporating the Qwen2 base model, aiming to bring GPT-4 level capabilities to visual instruction tuning. This repository provides a custom implementation that leverages the advanced features of Qwen2 for improved performance in large language and vision models. The project includes datasets for pretraining and finetuning, evaluation scripts, and a user-friendly interface. The result is a robust system designed for sophisticated multimodal tasks in various domains, particularly focused on financial applications with the FinVis dataset. Detailed instructions for installation, usage, and model evaluation are provided.}
}

@misc{yang2024insurancegpt,
  author       = {Yuzhe Yang and Haoqi Zhang and Zhidong Peng and Yilin Guo and Tianji Zhou},
  title        = {Travel Insurance Recommendation AI System Based on Flight Delay Predictions and Customer Sentiment},
  year         = {2024},
  code          = {https://github.com/TobyYang7/Travel-Insurance-Recommendation-AI-System},
  slides          = {https://cuhko365-my.sharepoint.com/:p:/g/personal/121020064_link_cuhk_edu_cn/EeZSiXgHQB9Dq-2wFqcVypUBuUREppF42pGmyjBsHWRVqw?e=6h7Zxc&nav=eyJzSWQiOjM1MCwiY0lkIjoyMDI3ODI4Nzk3fQ},
  pdf={Travel_Delay_Insurance_Recommendation_AI_System.pdf},
  preview={iba.png},
  note={Predicting Purchase Intentions for Dynamic Insurance Pricing},
  abstract={In this project, we designed an AI system to identify
potential travel insurance intentions of customers. Our designed
large language model (LLM), named Insurance-GPT, is capable
of analyzing in real-time during interactions with users and it
utilizes deep learning model to accurately predict flight delay. This
provides a good user experience, as well as provides a reference
for pricing strategies to insurance companies. The Insurance-GPT
can be downloaded at https://modelscope.cn/models/TobyYang7/
InsuranceGPT. Additionally, the complete source code for this
project is available on GitHub at https://github.com/TobyYang7/
Travel-Insurance-Recommendation-AI-System.}
}

@misc{flight_information_system,
  author       = {Yuzhe Yang and Zitong Wang and Baoyin Zhang and Haoqi Zhang and Jianzhen Chen and Zhidong Peng},
  title        = {Flight Information System},
  year         = {2024},
  preview={plane.gif},
  code = {https://github.com/tobyyang7/flight-information-system},
  abstract     = {The Flight Information System project aims to revolutionize data management in the aviation sector by developing a sophisticated relational database system. This system, grounded in an Entity-Relationship (E-R) model, is designed to streamline airline operations, including passenger bookings and flight logistics, ensuring data integrity and operational efficiency. By integrating Large Language Models (LLM), the project enhances the database architecture and query generation process, facilitating more intuitive interactions. The outcome is a functional, user-friendly web interface that replicates the complexities of airline management, offering both administrative and user-level interactions.},
  note         = {A project focused on enhancing aviation sector data management through a relational database system, integrated with LLM for optimized query generation and user-friendly web interface.},
}

@misc{light_pollution_ahp_2023,
  author       = {Yuzhe Yang and Jianzhen Chen and Zhongbin Chen},
  title        = {Evaluation Model of Light Pollution by Multi-conditional AHP},
  year         = {2023},
  preview={map_pop.jpg},
  pdf={Evaluation Model of Light Pollution by Multi-conditional AHP.pdf},
  note         = {Performed GIS data analysis and developed a mathematical model to evaluate light pollution, considering factors such as population data and regional income.},
  abstract={Artificial light, while convenient, poses significant risks, including disruption of circadian rhythms in wildlife and severe health consequences for humans. California’s escalating light pollution problem exemplifies the need for effective solutions. To address this, we developed a light pollution risk assessment model focusing on economic, demographic, and ecological impacts, using indicators such as GDP per capita, population density, and the biological abundance index. The model, weighted by the analytic hierarchy process (AHP), was tested in California’s diverse regions—protected areas, rural, suburban, and urban settings. Our findings suggest that interventions, particularly limiting light sources with specific wavelengths, can significantly reduce light pollution risk, especially in suburban areas, demonstrating a risk reduction of up to 2.63%. These results highlight the importance of targeted strategies in mitigating light pollution effectively.}
}

@misc{howard2022nfl,
  author       = {Yuzhe Yang},
  title        = {Kaggle Competition: 1st and Future - Player Contact Detection},
  year         = {2023},
  note = {Bronze Medal},
  abstract         = {Hosted by the National Football League (NFL), this competition aimed to detect player contacts using video and player tracking data to improve player safety.},
  website          = {https://kaggle.com/competitions/nfl-player-contact-detection},
  preview={kaggle2023.gif}
}
@misc{yang2023seo,
  author       = {Yuzhe Yang and Xiayu Ni and RongXin Cao},
  title        = {Game Theory Analysis of SEO Strategies: From Methods to Models},
  year         = {2023},
  note         = {This project examines SEO strategies through the lens of game theory, developing a model to enhance website visibility and ranking. The work involves both theoretical analysis and practical implementation, offering insights into effective SEO techniques.},
  abstract     = {This project delves into the application of game theory to Search Engine Optimization (SEO) strategies, focusing on enhancing a website’s importance score through various optimization techniques. The study includes the development and implementation of a customized optimization model, comparative analysis of different SEO methods, and validation through real-world system implementation. The findings demonstrate how specific SEO strategies can effectively improve a website's visibility and ranking in search engine results.},
  code          = {https://github.com/TobyYang7/EIE3280_Project},
  pdf = {EIE3280.pdf},
  preview={EIE3280.png}
}

@misc{yang2017FTC,
author = {RDFZ Robotic Team: Prototype},
title = {FIRST® Tech Challenge Game: RELIC RECOVERY},
year = {2017},
preview = {FTC2017.jpeg},
pdf={https://www.firstinspires.org/sites/default/files/uploads/resource_library/ftc/2017-2018/first-spectatorflyer18-ftc-ltr-dec-form.pdf}
}

@misc{yang2015FTC,
author = {JDFZ Robotic Team: UNICORN 1225},
title = {FIRST® Tech Challenge Game: RES-Q},
year = {2015},
preview = {FTC2016.jpeg},
pdf={https://www.firstinspires.org/sites/default/files/uploads/resource_library/ftc/first-res-q/first-resq-one-page.pdf}
}
