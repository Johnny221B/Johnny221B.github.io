<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Yuzhe Yang</title>
    <meta name="author" content="Yuzhe Yang" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" type="text/css" href="src/stylesheet.css" />
    <link rel="icon" href="./src/logo/CUHK_rm_bg.png" type="image/x-icon" />
    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-YE837RNHM5"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-YE837RNHM5");
    </script>
  </head>

  <body>
    <table class="main-container">
      <tbody>
        <tr class="zero-padding">
          <td class="zero-padding">
            <table class="content-table">
              <tbody>
                <tr class="zero-padding profile-row">
                  <td class="profile-section">
                    <p class="text-center">
                      <!-- <name>Yuzhe</name> <name-normal>(Toby)</name-normal> <name>Yang</name>  -->
                      <name>Yuzhe (Toby) Yang</name>
                    </p>
                    
                    <!-- Mobile Profile Image - Only visible on mobile -->
                    <div class="mobile-profile-container">
                      <div class="profile-container mobile-profile-size">
                        <div class="profile-card">
                          <div class="profile-front">
                            <video class="profile-video" autoplay muted loop playsinline>
                              <source src="./src/img/profile-hdr.mp4" type="video/mp4" />
                              Your browser does not support the video tag.
                            </video>
                          </div>
                          <div class="profile-back">
                            <img class="profile-img" alt="lego photo" src="./src/img/lego.gif" />
                          </div>
                        </div>
                      </div>
                      
                      <!-- Mobile Social Media Links -->
                      <div class="social-links">
                        <a href='https://scholar.google.com/citations?user=Oj296F8AAAAJ' class="social-link">
                          <img src="./src/logo/google-scholar.svg" alt="Google Scholar" class="social-icon">
                        </a>
                        <a href='https://github.com/TobyYang7' class="social-link">
                          <img src="./src/logo/github.svg" alt="GitHub" class="social-icon">
                        </a>
                        <a href='https://x.com/Toby_Yang_7' style="text-decoration: none;">
                          <img src="./src/logo/x.svg" alt="Twitter/X" class="social-icon">
                        </a>
                      </div>
                    </div>
                    
                    <p>
                      Hi, I am Toby Yang, also go by the name Yuzhe Yang (Èò≥Èõ®Âì≤). I am currently a final-year undergrad at
                      <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a>,
                      majoring in Computer Science and Engineering. During my undergrad studies at CUHK-Shenzhen, I have been fortunate to be advised
                      by <a href="https://wabyking.github.io/old.html">Prof. Benyou Wang</a>
                      and to collaborate closely with
                      <a href="https://sme.nju.edu.cn/yhh/list.psp"
                        >Prof. Honghai Yu</a
                      >‚Äôs team at
                      <a href="https://www.nju.edu.cn/en/">Nanjing University</a
                      >. I was also a member of
                      <a href="https://thefin.ai/">TheFinAI</a>, led by Jimin
                      Huang. Prior to that, I had the opportunity to work with
                      <a href="https://sds.cuhk.edu.cn/en/teacher/268"
                        >Prof. Jianfeng Mao</a
                      >.
                    </p>
                    <p>
                      My vision is to build reliable and trustworthy AI systems
                      that bridge the gap between machines and the real world.
                      My focus is on enabling (vision) language model to
                      interact more reliable with humans and their environments,
                      empowering them to understand and simulate the underlying
                      principles of the world. Additionally, I strive to uncover
                      the internal mechanisms of language model to demystify
                      their "black-box" nature, making AI system more interpretable,
                      transparent, and aligned with human values. Through my
                      research, I hope to contribute to the development of AI
                      systems that are not only intelligent but also
                      responsible, ethical, and deeply integrated into our daily
                      lives.
                    </p>
                    <p>
                      <b>Currently, I am seeking a PhD position.</b>
                      I am also open to research collaborations. If you are interested in my work or would like to discuss potential collaboration opportunities, please feel free to reach out and 
                      <link
                        href="https://assets.calendly.com/assets/external/widget.css"
                        rel="stylesheet"
                      />
                      <script
                        src="https://assets.calendly.com/assets/external/widget.js"
                        type="text/javascript"
                        async
                      ></script>
                      <a
                        href=""
                        onclick="Calendly.initPopupWidget({url: 'https://calendly.com/yuzheyang/30min'});return false;"
                      >schedule time</a>
                      with me.
                    </p>
                    <b>Email</b>:
                    <span id="email"></span>
                    <script>
                      const user = "yuzheyang";
                      const domain = "link.cuhk.edu.cn";
                      const email = user + "@" + domain;
                      const link = "mailto:" + email;
                    
                      const a = document.createElement("a");
                      a.href = link;
                      a.innerHTML = `<code>${email}</code>`;
                      document.getElementById("email").appendChild(a);
                    </script>
                    <br />
                    <br />
                  </td>
                  <td class="profile-image-section">
                  <div class="profile-container" style="margin-top: 40px">
                    <div class="profile-card">
                    <div class="profile-front">
                      <video class="profile-video" autoplay muted loop playsinline>
                        <source src="./src/img/profile-hdr.mp4" type="video/mp4" />
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <div class="profile-back">
                      <img class="profile-img" alt="lego photo" src="./src/img/lego.gif" />
                    </div>
                    </div>
                  </div>
                  
                  <!-- Social Media Links -->
                  <div class="social-links">
                    <a href='https://scholar.google.com/citations?user=Oj296F8AAAAJ' class="social-link">
                      <img src="./src/logo/google-scholar.svg" alt="Google Scholar" class="social-icon">
                    </a>
                    <a href='https://github.com/TobyYang7' class="social-link">
                      <img src="./src/logo/github.svg" alt="GitHub" class="social-icon">
                    </a>
                    <a href='https://x.com/Toby_Yang_7' style="text-decoration: none;">
                      <img src="./src/logo/x.svg" alt="Twitter/X" class="social-icon">
                    </a>
                  </div>
                  </td>
              </tbody>
            </table>

            <div id="menu">
              <div><a href="#publications">Publications</a></div>
              <div><a href="#experience">Experiences</a></div>
              <div><a href="./src/pdf/yuzhe_cv.pdf">Resume</a></div>
              <div><a href="https://tobyyang7.github.io/photo/">Gallery</a></div>
            </div>

            <!-- News Section -->
            <table class="content-table">
              <tbody>
                <tr>
                  <td class="section-padding">
                    <heading>News</heading>
                    <p>
                        <b>06/2025</b>: One paper (FDPT) was accepted to <i><a href="https://iccv.thecvf.com/">ICCV 2025</a></i>. See you in Hawaii! üå¥<br>
                      <b>05/2025</b>: Invited talk at <a href="https://mp.weixin.qq.com/s/fGJvmUHTk5dDzOhLdHsv_Q">Wisemodel.cn</a> (Special Session by FreedomAI Team in CUHK-Shenzhen).<br>
                      <b>04/2025</b>: <a href="https://tobyyang7.github.io/TwinMarket">TwinMarket</a> won the <a href="./src/img/best_paper.jpg"><b>Best Paper Award üèÜ</b></a> at the <i><a href="https://sites.google.com/view/financialaiiclr25/home">Advances in Financial AI Workshop</a> @ ICLR 2025</i>.<br>
                      <b>04/2025</b>: Invited talk at the <i><a href="https://sites.google.com/view/financialaiiclr25/home">Advances in Financial AI Workshop</a> @ ICLR 2025</i> and NUS in Singapore üá∏üá¨.<br>
                      <b>02/2025</b>: Our new work on socio-economic simulation, <a href="https://tobyyang7.github.io/TwinMarket">TwinMarket</a>, has been released.<br>
                      <b>01/2025</b>: One paper <a href="https://arxiv.org/abs/2410.14059">(UCFE-Benchmark)</a> was accepted to <i><a href="https://2025.naacl.org/">Findings of NAACL 2025</a></i>.<br>
                      <b>08/2024</b>: A financial foundation model, <a href="https://huggingface.co/TheFinAI/FinLLaMA-instruct">FinLLaMA-8B</a>, and a multimodal model, <a href="https://huggingface.co/TheFinAI/FinLLaVA">FinLLaVA-8B</a>, were released.<br>
                      <b>03/2024</b>: My first publication <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524001040?via%3Dihub">(FAST-CA)</a> was accepted to <i>Information Fusion 2024</i>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Section -->
            <table class="content-table">
              <tbody>
                <tr>
                  <td class="section-padding">
                    <heading>Research</heading>
                    <p>
                      Previously, my research was focused on:<br />
                      <b>(1) Language Model:</b> Human-AI Interaction <a href="https://arxiv.org/abs/2410.14059">(UCFE-Benchmark)</a>, Agent System <a href="https://tobyyang7.github.io/TwinMarket">(TwinMarket)</a>, AI for Scientific Applications <a href="https://arxiv.org/abs/2408.11878">(Open-FinLLMs)</a>, Trustworthy NLP <a href="https://arxiv.org/abs/2505.15634">(SAE-free)</a><br />
                      <b>(2) Data Mining:</b> Spatial-Temporal Modeling <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524001040?via%3Dihub">(FAST-CA)</a>, Social Computing
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

<!-- Publications Section -->
<table id="publications" class="content-table">
  <tbody>
    <tr>
      <td class="section-padding">
        <heading>Publications</heading>
        <p style="color: gray;">(* indicates equal contribution; ‚Ä† indicates corresponding author)</p>

        <table class="content-table">
          
          <tr>
            <td colspan="2" class="year-divider">
              <hr>
              <span class="year-label">2025</span>
            </td>
          </tr>
          
          <!-- TwinMarket Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/TwinMarket.jpg" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</papertitle>
              <br />
              <b>Yuzhe Yang*</b>, Yifei Zhang*, Minghao Wu*, Kaidi Zhang, Yunmiao Zhang, Honghai Yu‚Ä†, Yan Hu, Benyou Wang‚Ä†
              <br />
              <br />
              <em>{<a href="https://sites.google.com/view/financialaiiclr25/home">Financial AI</a>; <a href="https://sites.google.com/view/worldmodel-iclr2025/">World Models</a>} @ ICLR 2025 Workshop</em>
              <br />
              <a href="./src/img/best_paper.jpg"><b>Best Paper Award</b></a>, ICLR 2025 Workshop on Advances in Financial AI
              <br />
              <b>Travel Grant Award</b>, ICLR 2025 Workshop on Advances in Financial AI
              <br />
              A multi-agent framework that leverages LLMs to simulate socio-economic systems
              <br />
              <a href="https://arxiv.org/abs/2502.01506">Paper</a> /
              <a href="https://github.com/TobyYang7/TwinMarket">Code</a> /
              <a href="https://tobyyang7.github.io/TwinMarket">Project Page</a>
              <p></p>
            </td>
          </tr>

          <!-- UCFE Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/UCFE.png" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</papertitle>
              <br />
              <b>Yuzhe Yang*</b>, Yifei Zhang*, Yan Hu*, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie‚Ä†, Jimin Huang, Honghai Yu‚Ä†, Benyou Wang‚Ä†
              <br />
              <br />
              <em>Findings of NAACL 2025</em>
              <br />
              <a href="https://huggingface.co/papers?date=2024-10-21">#1 Paper of the day on Huggingface</a>
              <br />
              A User-Centric framework designed to evaluate LLMs' ability to handle complex financial tasks
              <br />
              <a href="https://aclanthology.org/2025.findings-naacl.300/">Paper</a> /
              <a href="https://github.com/TobyYang7/UCFE-Benchmark">Code</a> /
              <a href="https://huggingface.co/datasets/TobyYang7/UCFE">Dataset</a>
              <p></p>
            </td>
          </tr>

          <!-- FDPT Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/FDPT.png" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>FDPT: Federated Discrete Prompt Tuning for Black-Box Visual-Language Models</papertitle>
              <br />
              Jiaqi Wu, Simin Chen, Jing Tang, <b>Yuzhe Yang</b>, Yiming Chen, Lixu Wang, Song Lin, Zehua Wang, Wei Chen, Zijian Tian
              <br />
              <br />
              <em>ICCV 2025</em>
              <br />
              Federated prompt tuning approach for black-box visual-language models
              <br />
              <a href="#">Paper</a>
              <p></p>
            </td>
          </tr>

        <!-- SAE-free -->
        <tr class="publication-row">
          <td class="publication-image">
            <img src="./src/img/sae_free.png" width="100%" />
          </td>
          <td class="publication-content">
            <papertitle>Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models</papertitle>
            <br />
            Zihao Li, Xu Wang, <b>Yuzhe Yang</b>, Ziyu Yao, Haoyi Xiong, Mengnan Du‚Ä†
            <br />
            <br />
            <em>arXiv preprint 2025</em>
            <br />
            Enhance LLM reasoning by steering activations via a novel SAE-free method using Chain-of-Thought features, without external data
            <br />
            <a href="https://arxiv.org/abs/2505.15634">Paper</a>
            <p></p>
          </td>
        </tr>

          <tr>
            <td colspan="2" class="year-divider">
              <hr>
              <span class="year-label">2024</span>
            </td>
          </tr>

          <!-- Open-FinLLMs Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/finllava.png" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</papertitle>
              <br />
              Jimin Huang, Mengxi Xiao, Dong Li, Zihao Jiang, <b>Yuzhe Yang</b>, Yifei Zhang, Lingfei Qian, Yan Wang, Xueqing Peng, Yang Ren, Ruoyu Xiang, Zhengyu Chen, Xiao Zhang, Yueru He, Weiguang Han, Shunian Chen, Lihang Shen, Daniel Kim, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Guojun Xiong, Zhiwei Liu, Zheheng Luo, Zhiyuan Yao, Ruey-Ling Weng, Meikang Qiu, Kaleb E Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jian-Yun Nie, Jordan W. Suchow, Xiao-Yang Liu‚Ä†, Benyou Wang‚Ä†, Alejandro Lopez-Lira‚Ä†, Qianqian Xie‚Ä†, Sophia Ananiadou, Junichi Tsujii
              <br />
              <br />
              <em>arXiv preprint 2024</em>
              <br />
              First open-source financial multimodal LLM: FinLLaVA-8B
              <br />
              <a href="https://arxiv.org/abs/2408.11878">Paper</a> /
              <a href="https://huggingface.co/TheFinAI/FinLLaVA">Model</a>
              <p></p>
            </td>
          </tr>

          <!-- FAST-CA Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/FAST-CA.jpg" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction</papertitle>
              <br />
              Chi Li, Xixian Qi, <b>Yuzhe Yang</b>, Zhuo Zeng,Lianmin Zhang, Jianfeng Mao‚Ä†
              <br />
              <br />
              <em>Information Fusion 2024</em>
              <br />
              SOTA spatio-temporal model for predicting airport network delay propagation
              <br />
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524001040">Paper</a>
              <p></p>
            </td>
          </tr>

          <!-- FedDTPT Publication -->
          <tr class="publication-row">
            <td class="publication-image">
              <img src="./src/img/feddtpt.png" width="100%" />
            </td>
            <td class="publication-content">
              <papertitle>FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models</papertitle>
              <br />
              Jiaqi Wu, Simin Chen, <b>Yuzhe Yang</b>, Yijiang Li, Shiyue Hou, Rui Jing, Zehua Wang, Wei Chen, Zijian Tian
              <br />
              <br />
              <em>arXiv preprint 2024</em>
              <br />
              A federated prompt tuning method for black-box LLMs, enhancing privacy, efficiency, and performance on non-iid data
              <br />
              <a href="https://arxiv.org/abs/2411.00985">Paper</a>
              <p></p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </tbody>
</table>

<!-- Projects Section -->
<table class="content-table">
  <tbody>
    <tr>
      <td class="section-padding">
        <heading>Projects</heading>
        <p>
          <a href="https://github.com/ValueByte-AI/Awesome-LLM-in-Social-Science">Awesome LLM in Social Science</a><br>
          <a href="https://github.com/TobyYang7/Llava_Qwen2">LLaVA-Qwen2: Enhanced with Qwen2 Base Model</a><br>
          <a href="https://github.com/TobyYang7/Black-Box-Tuning">DeepSeek and LLaMA Implementation for Black-Box-Tuning (ICML 2022)</a><br>
          <a href="https://github.com/TobyYang7/Quant-GPT">Quant-GPT: Money is All You Need</a><br>
          <a href="https://github.com/TobyYang7/Travel-Insurance-Recommendation-AI-System">Travel Insurance Recommendation AI System</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>


<!-- Experiences -->
<table id="experience" class="content-table">
  <tbody>
    <tr>
      <td class="section-padding">
        <heading>Experiences
        </heading>
        <p>
        <!-- CUHK NLP Group -->
        <div class="experience-item">
          <!-- CUHK Logo -->
          <img src="./src/logo/CUHK.svg" alt="CUHK Logo" class="experience-logo" />
          <div class="experience-content">
            <b>CUHK-Shenzhen NLP Group (FreedomAI)</b>
            <span class="experience-date">2024.06 ‚Äì Present</span>
            <br />Undergraduate Research Assistant
            <br />
            Advised by <a href="https://wabyking.github.io/old.html">Prof. Benyou Wang</a> and Dr. Yan Hu
            <br />
            <span class="experience-note"><a href="https://mp.weixin.qq.com/s/ofw1yKqk62Vp8tbRUmXfQw" class="experience-note">Best Paper Award & Travel Grant Award (ICLR 2025 Workshop)</a></span>
          </div>
        </div>

        <!-- Nanjing University -->
        <div class="experience-item">
          <!-- NJU Logo -->
          <img src="./src/logo/NJU.svg" alt="NJU Logo" class="experience-logo" />
          <div class="experience-content">
            <b>School of Management & Engineering, Nanjing University</b>
            <span class="experience-date">2024.08 ‚Äì 2025.01</span>
            <br />Undergraduate Research Assistant
            <br />
            Advised by <a href="https://sme.nju.edu.cn/yhh/list.psp">Prof. Honghai Yu</a>
          </div>
        </div>

        <!-- TheFinAI -->
        <div class="experience-item">
          <!-- Placeholder Logo -->
          <img src="./src/logo/finai.png" alt="TheFinAI Logo" class="experience-logo" />
          <div class="experience-content">
            <b>TheFinAI</b>
            <span class="experience-date">2024.06 ‚Äì 2024.10</span>
            <br />Researcher
            <br />
            Led by <a href="https://thefin.ai/">Jimin Huang</a> and <a href="https://qianqian-xie.github.io/">Prof. Qianqian Xie</a>
          </div>
        </div>

        <!-- CUHK Data Science -->
        <div class="experience-item">
          <!-- Placeholder Logo -->
          <img src="./src/logo/CUHK.svg" alt="CUHK Logo" class="experience-logo" />
          <div class="experience-content">
            <b>School of Data Science, CUHK-Shenzhen</b>
            <span class="experience-date">2023.08 ‚Äì 2024.06</span>
            <br />Undergraduate Research Assistant
            <br />
            Advised by <a href="https://sds.cuhk.edu.cn/en/teacher/268">Prof. Jianfeng Mao</a>
            <br />
            <span class="experience-note">Undergraduate Research Award (2024, 2025)</span>
          </div>
        </div>

      </td>
    </tr>
  </tbody>
</table>


<!-- Services -->
<table class="content-table">
  <tbody>
    <tr>
      <td class="section-padding">
        <heading>Services</heading>
        <p>
          <b>Reviewer:</b> IJCAI 2025, ICLR 2025 Workshop, ACL 2025 SRW
        </p>
      </td>
    </tr>
  </tbody>
</table>

            <!-- Miscellaneous Section -->
            <table class="content-table">
              <tbody>
                <tr>
                  <td class="section-padding">
                    <heading>Miscellaneous</heading>
                    <p>
                      üì∏ I am an amateur photographer with an interest in
                      digital, film, and aerial photography, with a special
                      passion for landscape photography üèîÔ∏è. You can find my
                      photos on
                      <a href="https://unsplash.com/@tobyyang">Unsplash</a> üéûÔ∏è.
                      Also, you can visit my
                      <a href="https://tobyyang7.github.io/photo"
                        >HDR photo gallery</a
                      >.
                    </p>
                    <img src="src/img/preview.png" width="100%" />
                    <br />
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Footer Section -->
            <table class="content-table">
              <tbody>
                <tr>
                  <td class="zero-padding">
                    <br />
                    <div class="footer-center">
                      <script
                        type="text/javascript"
                        id="clustrmaps"
                        src="//cdn.clustrmaps.com/map_v2.js?cl=5ab2ff&w=300&t=tt&d=xoDg7VaCrLXmmLZN9tIoONrsj5uu62Bt8Fx4GVA9cuM&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353"
                      ></script>
                    </div>
                    <p class="footer-text">
                      2024-2025 ¬©Ô∏è Yuzhe Yang
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
